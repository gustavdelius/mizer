---
title: "Estimating feeding kernels from stomach data"
author: "Gustav Delius"
date: 2019-08-01
output:
  html_document:
    toc: true
    toc_depth: 4
    toc_float: true
    code_folding: hide
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(ggplot2)
library(bbmle)
library(mizer)
```

## Introduction
We want to use available observations of the size distribution of prey items in
predator stomachs to estimate the feeding kernels to use in size-spectrum
models. 

We will initially use data for sardine and anchovy provided by Mariella and the
[Barnes
dataset](http://www.esajournals.org/doi/abs/10.1890/07-1551.1?journalCode=ecol).
Later we will want to extend that to the [DAPSTOM
dataset](https://www.cefas.co.uk/media/41463/dapstom-phase-4-report-2014-dlm.pdf)
and a [dataset of diets of North Atlantic
fishes](http://www.fishecology.org/diets/diets.htm)

## Density functions
In order to avoid confusion, I will discuss in detail the different ways in
which we could describe the size distribution of the stomach items in terms of
different density functions. 

### Prey item distribution
We can describe the size distribution of prey in stomachs of predators of size
$w'$ by the density function $N_w(w|w')$. This is a density in $w$, so that the
number of prey items with a size between $w$ and $w+dw$ is $N_w(w|w')dw$.

Alternatively we can describe the size distribution by the probability density
that a random prey item in a stomach of a predator of size $w'$ has size $w$.
This probability density function is simply the normalised version of the number
density:
$$
f_w(w|w') = \frac{N_w(w|w')}{z(w')},
$$
where the normalisation factor $z(w')$ is the total number of prey items,
$$z(w') = \int N_w(\tilde{w}|w')d\tilde{w}.$$
The sizes of the actual observed prey items in the data set are seen as samples
from this probability distribution.

Rather than working with the prey size $w$, we can also work with the
predator/prey mass ratio $r = w' / w$. The probability distribution of this is 
$$
f_r(r|w') = \left|\frac{dw}{dr}\right| f_w(r|w') = \frac{w^2}{w'} f_w(w|w').
$$

It is actually a good idea to work with the logarithm of the predator/prey mass
ratio $l = \log(r) = \log(w'/w)$. Its distribution is
$$
f_l(l|w') = \left|\frac{dw}{dl}\right| f_w(w|w') = w f_w(w|w').
$$
We will be making the fundamental assumption that the distribution of the
predator/prey mass ratio is independent of the predator size: 
$f_r(r|w') = f_r(r)$ (and hence also $f_l(l|w') = f_l(l)$). Of course we
should look at the data to see if this assumption is reasonable.

### Biomass distribution
We would also be interested in how the prey biomass is distributed over the prey
sizes, because as far as the predator is concerned, it is the amount of biomass
that counts, not the number of individuals that are needed to make up that
biomass. So instead of the number density $N_w(w|w')$, we would look at the
biomass density $B_w(w|w')$, defined so that $B_w(w|w')dw$ is the total biomass
of prey items with sizes between $w$ and $w+dw$. The number density and the
biomass density are simply related:
$$
B_w(w|w') = w\ N_w(w|w').
$$
Again there is a probability density associated to this:
$$
b_w(w|w') = \frac{B_w(w|w')}{\int B_w(\tilde{w}|w')d\tilde{w}}.
$$
This gives the probability that a randomly chosen unit of biomass in the stomach
of a predator of size $w'$ comes from a prey item of size $w$. The relation to
the earlier density is
$$
b_w(w|w') \propto w\ f_w(w|w').
$$

To see how this biomass density is related to the observations of stomach
contents, consider that instead of recording one observation for each prey
individual, we could in principle record one observation for each unit of
biomass in the stomach. This would correspond to splitting each single
observation for a prey item of size $w$ into $w$ separate observations. This new
larger set of observations could then be seen as a sample from the distribution
described by the biomass density $b_w(w|w')$.

Again we can transform the probability density to different variables. In
particular the density as a function of $l = \log(w'/w)$ is
$$
b_l(l|w') = w\,b_w(w|w') \propto w^2\,f_w(w|w')
\propto w\,f_l(l) \propto e^{-l}f_l(l).
$$
So again $b_l(l|w')$ is independent of $w'$.

## Mariella's dataset

Mariella has data of stomach content for anchovy and sardine. Mariella can
provide more details of the origin of this data.
```{r message=FALSE, warning=FALSE}
stomach <- read_csv("../humboldt/Data_PPMR.csv")
stomach
```

The `wprey` variable is the typical weight of an individual of a particular prey
species and `Nprey` is the average number of individuals of that species in a
stomach and `wpredator` is the weight of the predator. The table reports the
same predator size for all anchovy and the same predator size for all sardine
because all predators of each species had very similar sizes.

We capitalise the species names and add a column for the log of the
predator/prey mass ratio $l = \log(w'/w)$.
```{r}
stomach <- stomach %>% 
    mutate(Species = str_to_title(Species),
         l = log(wpredator / wprey))
stomach
```

Let's look at the smallest and largest prey items as well as the log of the
smallest and largest predator/prey mass ratios in the data:
```{r}
stomach %>% 
    group_by(Species) %>% 
    summarise(wprey_min = min(wprey),
              wprey_max = max(wprey),
              lmin = min(l),
              lmax = max(l))
```
We can not say for sure that anchovy and sardine do not eat larger prey than
contained in the dataset, because large prey are rare and even the observed
large prey only hav a very small count:
```{r}
stomach %>% 
    group_by(Species) %>% 
    filter(wprey == max(wprey))
```
Remember that `Nprey` is the average number of prey individuals of that size in
a stomach. So the largest prey item observed in an anchovy stomach only showed
up in one of 10.000 stomachs. One would need a very large sample of stomachs to
have a good chance to observe any larger prey. So to get a better value for the
largest possible prey one should look at the morphology of the predator species.

Similarly, we can not be certain of the true size of the smallest possible prey
items because it may just have been to difficult to identify cells of the
smallest plankton prey, especially as they will be digested very quickly. Again
it may make sense to complement this study by a look at the morphology of the
gill rakers, although one has to acknowledge that gill rakers can, when they
become clogged, filter out smaller individuals than would be indicated by the
spacing of the rakers.

We will have more discussion of the relation between the stomach data and the
actual diet of the predator, taking digestion into account, later. For now we
concentrate on describing the stomach data.

There are two ways to visualise the size-distribution of the prey items: we can
bin the data and plot histograms, or we can plot density kernel estimates. We'll
start with a look at binned data.

### Histograms
We can estimate the probability densities from the data by binning the data in
equally sized bins of the relevant variable and plotting histograms of the
number of prey items in each bin. 

We use size bins that are equally-spaced on a logarithmic axis between the
smallest and largest predator/prey size ratio. For later use we create a vector
`breaks` with the bin boundaries:
```{r}
no_bins <- 30  # Number of bins
binsize <- (max(stomach$l) - min(stomach$l)) / (no_bins - 1)
breaks <- seq(min(stomach$l) - binsize/2,
              by = binsize, length.out = no_bins + 1)
```
To get the correct normalisation of the densities, the height of the bars should
be equal to the number of prey items divided by the width of the bin divided by
the total number of prey items. That way the area under the histogram will be
equal to 1. We also add a column `l` with the log of predator/prey mass ratio at
the centre of the each bin.
```{r}
binned_stomach <- stomach %>% 
    # bin data
    mutate(cut = cut(l, breaks = breaks, right = FALSE,
                     labels = FALSE)) %>% 
    group_by(Species, cut) %>% 
    summarise(Numbers = sum(Nprey), 
              Biomass = sum(Nprey * wprey)) %>% 
    # normalise
    mutate(Numbers = Numbers / sum(Numbers) / binsize,
           Biomass = Biomass / sum(Biomass) / binsize)  %>%
    # column for predator/prey size ratio
    mutate(l = map_dbl(cut, function(idx) breaks[idx] + binsize/2))
binned_stomach
```
We convert this into the long table format preferred by ggplot2.
```{r}
binned_stomach <- binned_stomach %>%
gather(key = "Type", value = "Density", Numbers, Biomass)
```
We can now easily plot the histograms that represent estimates of the normalised
number density $f_l(l)$ and the normalised biomass density $b_l(l)$.
```{r}
binned_stomach %>% 
  ggplot(aes(l, Density, fill = Type)) +
    geom_col(position = "dodge") +
    facet_grid(rows = vars(Species), scales = "free_y") +
    xlab("Log of Predator/Prey mass ratio") +
    expand_limits(x = c(0, 30))
```

Note how the biomass of prey is concentrated at very different predator/prey
mass ratios than the number of prey. The information about the biomass
distribution is contained mostly in the left tail of the number density and,
vice versa, the information about the number distribution is mostly contained in
the right tail of the biomass density.

### Kernel density estimation
An alternative way to estimate densities from data is provided by the kernel
density estimation method. Here, rather than aggregating observations in bins,
each observation contributes its own little Gaussian centred at that data value.
The estimated density function is obtained by summing all these little 
Gaussians together. That produces a smoother estimate than the histogram method.
The width of the individual Gaussians plays the same role as the bins size.
There are some heuristics that allow R to choose a sensible width that provides
a good summary of the data. We decrease that bandwidth by a factor of 1/2 to get
a slightly more detailed view:
```{r}
adjust <- 1/2  # decrease bandwidth for kernel estimate
stomach <- stomach %>% 
    group_by(Species) %>% 
    mutate(weight_numbers = Nprey / sum(Nprey),
           weight_biomass = Nprey * wprey / sum(Nprey * wprey))
ggplot(stomach) +
    geom_density(aes(l, weight = weight_numbers,
                     fill = "Numbers"),
                 adjust = adjust) +
    geom_density(aes(l, weight = weight_biomass,
                     fill = "Biomass"),
                 adjust = adjust) +
    facet_grid(rows = vars(Species), scales = "free_y") +
    xlab("Log of predator/prey mass ratio") +
    expand_limits(x = c(0, 29))
```

This idea of spreading out each observation into a little Gaussian is
particularly appropriate for this dataset, where the size of the observed prey
item is only estimated from the species identity of the prey item and where also
the predator size is not know exactly. Both of these lead to uncertainty in the
predator/prey size ratio.

### Gaussian does not fit
Let us start by observing that it would be very inappropriate to use a Gaussian
curve to describe these densities. We illustrate this in the case of the
Anchovy. We plot the normal distribution with the same mean and standard
deviation as the data on top of the histogram for the number distribution.
```{r}
chosen_species = "Anchovy"
weighted.sd <- function(x, w) {
  sqrt(sum(w * (x - weighted.mean(x, w))^2))
}
fit <- stomach %>% 
  filter(Species == chosen_species) %>% 
  summarise(mean = weighted.mean(l, weight_numbers),
            sd = weighted.sd(l, weight_numbers))
stomach %>% 
  filter(Species == chosen_species) %>% 
  ggplot() +
    geom_density(aes(l, weight = weight_numbers),
                 fill = "#00BFC4", adjust = adjust) +
    xlab("Log of Predator/Prey mass ratio") +
    stat_function(fun = dnorm, 
                  args = list(mean = fit$mean, 
                              sd = fit$sd), 
                  colour = "blue") +
    expand_limits(x = c(6, 30))
```

This may not look so bad. However, if the size distribution was correctly
described by the normal distribution, i.e., if
$$
f_l(l) \propto \exp\left(-\frac{(l-\mu)^2}
  {2\sigma^2}\right),
$$
then the biomass density would be given by
$$
\begin{split}
b_l(l) &\propto e^{-l}f_l(l)
\propto \exp\left(-\frac{(l-\mu)^2}
  {2\sigma^2} - l\right)\\
&\propto \exp\left(-\frac{(l-(\mu-\sigma^2)^2}
  {2\sigma^2}\right)
\end{split}
$$
Thus the biomass would also be normally distributed with the mean
shifted by $\sigma^2$. Putting this into the same picture gives
```{r}
stomach %>% 
  filter(Species == chosen_species) %>% 
  ggplot() +
    geom_density(aes(l, weight = weight_numbers,
                     fill = "Numbers"),
                 adjust = adjust) +
    geom_density(aes(l, weight = weight_biomass,
                     fill = "Biomass"),
                 adjust = adjust) +
    xlab("Log of Predator/Prey mass ratio") +
    stat_function(fun = dnorm, 
                  args = list(mean = fit$mean, 
                              sd = fit$sd), 
                  colour = "blue") +
    stat_function(fun = dnorm, 
                  args = list(mean = fit$mean - fit$sd^2, 
                              sd = fit$sd), 
                  colour = "red") +
    expand_limits(x = c(0, 30))
```

This does not fit the actual biomass distribution at all! 

For the Sardine the normal distribution fits neither the number density
nor the biomass density.
```{r}
chosen_species = "Sardine"
weighted.sd <- function(x, w) {
  sqrt(sum(w * (x - weighted.mean(x, w))^2))
}
fit <- stomach %>% 
  filter(Species == chosen_species) %>% 
  summarise(mean = weighted.mean(l, weight_numbers),
            sd = weighted.sd(l, weight_numbers))
stomach %>% 
  filter(Species == chosen_species) %>% 
  ggplot() +
    geom_density(aes(l, weight = weight_numbers,
                     fill = "Numbers"),
                 adjust = adjust) +
    geom_density(aes(l, weight = weight_biomass,
                     fill = "Biomass"),
                 adjust = adjust) +
    xlab("Log of Predator/Prey mass ratio") +
    stat_function(fun = dnorm, 
                  args = list(mean = fit$mean, 
                              sd = fit$sd), 
                  colour = "blue") +
    stat_function(fun = dnorm, 
                  args = list(mean = fit$mean - fit$sd^2, 
                              sd = fit$sd), 
                  colour = "red") +
    expand_limits(x = c(0, 30))
```


### Exponential fits better
We have learned above that the small number of large individuals in the
stomach provide the dominant contribution to the biomass consumed by
the predator and hence are important. To make them more visible, we show
the histograms again but now with a logarithmic y-axis
```{r}
binned_stomach %>% 
  ggplot(aes(l, Density)) +
    geom_col() +
    facet_grid(Species ~ Type, scales = "free_y") +
    xlab("Log of Predator/Prey mass ratio") +
    scale_y_continuous(trans = "log")
```

This motivates us to try to fit a truncated exponential distribution to the
data (which would be a truncated power-law distribution when working with the
predator/prey mass ratio instead of its logarithm).

```{r}
binned_stomach %>% 
  ggplot(aes(l, Density)) +
    geom_col() +
    geom_smooth(method = "lm", se = FALSE) +
    facet_grid(Species ~ Type, scales = "free_y") +
    xlab("Log of Predator/Prey mass ratio") +
    scale_y_continuous(trans = "log")
```

Of course again we have the problem that the biomass density is not independent
of the number density, so fitting separate distributions to the two is not
appropriate. If we assume that the number distribution follows the exponential
distribution, i.e.,
$$
f_l(l)\propto \exp(\alpha\ l)
$$
for some parameter $\alpha$ then the biomass distribution follows an
exponential distribution with parameter $\alpha - 1$:
$$
b_l(l) = e^{-l}f_l(l)
\propto \exp((\alpha - 1)l).
$$
Amazingly, the parameters of the independently fitted distributions do almost
perfectly have this relationship:
```{r}
binned_stomach %>% 
  group_by(Species, Type) %>% 
  group_modify(~ broom::tidy(lm(log(Density) ~ l, data = .x))) %>% 
  filter(term == "l")
```
The parameter of the biomass distribution is almost exactly 1 less than the parameter of the numbers distribution.

### Maximum likelihood fit
Fitting the exponential distribution by fitting a linear model to the logged
histogram is not really appropriate. A better method would be to fit directly to
the data using the maximum likelihood principle.

#### Number distribution
The density of the truncated exponential distribution is
$$
f_l(l) = \begin{cases}
\frac{1}{Z}\exp(\alpha\ l) &\text{ if }l\in[l_{min}, l_{max}]\\
0 &\text{ otherwise,}
\end{cases}
$$
where
$$
Z = \int_{l_{min}}^{l_{max}}\exp(\alpha\ l)dl=
\frac{1}{\alpha}\left(\exp(\alpha\ l_{max})-\exp(\alpha\ l_{min})\right).
$$

Due to the way the parameters $l_{min}$ and $l_{max}$ enter in the normalisation
of the probability density, the maximum likelihood will be attained at the
largest possible value for $l_{min}$ and the smallest possible value for
$l_{max}$, i.e., they are determined by the smallest and largest value appearing
in the dataset.

It remains only to estimate the rate parameter $\alpha$. The log likelihood
function is
$$
\begin{split}
l(\alpha) &= \sum_i\left(\alpha\ l_i 
- \log\left(\frac{1}{\alpha}\left(\exp(\alpha\ l_{max})-\exp(\alpha\ l_{min})\right)\right)\right)\\
&=\alpha \sum_il_i + n\log|\alpha|
- n\log\left|\exp(\alpha\ l_{max})-\exp(\alpha\ l_{min})\right|
\end{split}
$$

In order to avoid having to minimize this numerically, we now make the 
approximation that $\alpha$ is large enough so that $\exp(\alpha\ l_{min})$ is 
negligibly small compared to $\exp(\alpha\ l_{max})$. Then
$$
l(\alpha)\approx\alpha \sum_il_i + n\log(\alpha) 
- n\ \alpha\ l_{max}
$$
To find the value of $\alpha$ that maximizes this likelihood we set
$$
0=\frac{dl(\alpha)}{d\alpha} \approx \sum_il_i+\frac{n}{\alpha}-n\ l_{max}
$$
and solve for $\alpha$. This gives
$$
\alpha \approx \frac{1}{l_{max}-\bar{l}}
$$
where $\bar{l}$ is the average over all $l_i$.

```{r}
est <- stomach %>% 
  group_by(Species) %>% 
  summarise(lbar = weighted.mean(l, weight_numbers),
            lmax = max(l),
            lmin = min(l),
            alpha = 1/(lmax - lbar))
est
```
Note how sensitive these estimates are to the value of `lmax`. To see why these are not good estimates, let us have a look what the fit looks like.
```{r}
dnumbers <- function(l, alpha, lmax) {
  d <- as.numeric(l <= lmax)
  d[d > 0] <- dexp(lmax - l[d > 0], alpha)
  return(d)
}
dbiomass <- function(l, alpha, lmin) {
  d <- as.numeric(l >= lmin)
  d[d > 0] <- dexp(l[d > 0] - lmin, 1 - alpha)
  return(d)
}
selected_species <- "Sardine"
binned_stomach %>% 
  filter(Species == selected_species) %>% 
  ggplot() +
    geom_col(aes(l, Density, fill = Type)) +
    stat_function(fun = dnumbers, 
                  args = list(alpha = est$alpha[est$Species == selected_species], 
                              lmax = est$lmax[est$Species == selected_species]), 
                  colour = "blue") +
    stat_function(fun = dbiomass, 
                  args = list(alpha = est$alpha[est$Species == selected_species], 
                              lmin = est$lmin[est$Species == selected_species]), 
                  colour = "red") +
    xlab("Log of Predator/Prey mass ratio")  +
    expand_limits(x = c(0, 30))
```

Clearly the density should not rise exponentiall all the way to the largest
observed value `lmax` but there should be a smoother cutoff. There should also
be a smoother cutoff at `lmin`.

#### Biomass distribution
If $f_l(l)$ is given by the truncated exponential distribution with rate
parameter $\alpha$ then the biomass distribution $b_l(l)$ is given by the
truncated exponential distribution with rate parameter $\alpha_B = \alpha - 1$.
If this is negative enough that $\exp(\alpha_B\ l_{max})$ is negligibly
small compared to $\exp(\alpha_B\ l_{min})$, then
$$
l(\alpha_B)\approx\alpha_B \sum_iw_il_i 
+ \sum_iw_i\left(\log(-\alpha_B) 
- \alpha_B\ l_{min}\right)
$$
To find the value of $\alpha$ that maximizes this likelihood we set
$$
0=\frac{dl(\alpha_B)}{d\alpha_B} \approx \sum_iw_il_i
+\sum_iw_i\left(\frac{1}{\alpha_B}-\ l_{min}\right)
$$
and solve for $\alpha_B$. This gives
$$
\alpha_B \approx \frac{1}{l_{min} - \bar{l}_w}
$$
where $\bar{l}_w$ is the weighted mean
$$
\bar{l}_w = \frac{\sum_iw_il_w}{\sum_iw_i}.
$$


```{r}
est2 <- stomach %>% 
  group_by(Species) %>% 
  summarise(lbar = weighted.mean(l, weight_numbers),
            lmax = max(l),
            lmin = min(l),
            alpha = 1/(lmax - lbar),
            lbarw = weighted.mean(l, weight_biomass),
            alpha_B = 1 / (lmin - lbarw),
            diff = alpha - alpha_B
            ) 
est2 %>% select(Species, alpha, alpha_B, diff)
```
The difference between $\alpha$ and $\alpha_B$ should be 1. This works quite
well for anchovy, but fails dramatically for sardine. The failure is not
surprising: the estimate for $\alpha_B$ depends sensitively on $l_{min}$, which
the maximum likelihood principle told us to choose to be equal to the smallest
observed $l_i$. However, due to the small number of observations with small
values for $l$, it may well be that the true smallest value for $l$
is smaller than the smallest observed and has simply by chance not been detected.

A natural question to ask therefore is if with a different choice for $l_{min}$ 
the estimate for $\alpha_B$ would agree with our estimate for $\alpha - 1$.
So we set $\alpha_B = \alpha - 1$ and solve for $l_{min}$:
$$
l_{min} = \bar{l}_w + \frac{1}{\alpha - 1}
$$
```{r}
est3 <- stomach %>% 
  group_by(Species) %>% 
  summarise(lbar = weighted.mean(l, weight_numbers),
            lmax = max(l),
            lmin = min(l),
            alpha = 1/(lmax - lbar),
            lbarw = weighted.mean(l, weight_biomass),
            lmin_B = lbarw + 1 / (alpha - 1),
            diff = lmin - lmin_B
            ) 
est3 %>% select(Species, lmin, lmin_B, diff)
```
So the number distribution and the biomass distribution would be consistent 
with each other if the actual $l_{min}$ for sardine was just a little bit 
smaller than the smallest observed one. This is not unreasonable. 

### Sigmoidal cutoffs
The simplest way to implement smoother cutoffs at the ends of the distributions
is to multiply by two sigmoidal functions: a rising one centred near `lmin`
and a falling one near `lmax`. So we choose
$$
f_l(l) \propto \frac{\exp(\alpha\ l)}
{\left(1+e^{u_l(l_l - l)}\right)
\left(1+e^{u_r(l - l_r)}\right)}.
$$
```{r}
fl <- function(l, alpha, ll, ul, lr, ur) {
  exp(alpha * l) /
    (1 + exp(ul * (ll - l))) /
    (1 + exp(ur * (l - lr))) 
}
dtexp <- function(l, alpha, ll, ul, lr, ur) {
  d <- fl(l, alpha, ll, ul, lr, ur) /
    integrate(fl, 0, 30, alpha = alpha, 
              ll = ll, ul = ul, lr = lr, ur = ur)$value
  if (any(d <= 0)) {
    stop("The density contains non-positive values when",
         " alpha = ", alpha, " ll = ", ll, " ul = ", ul,
         " lr = ", lr, " ur = ", ur)
  }
  return(d)
}
```

We have introduced 4 new parameters into the distribution: $l_l$ is the log of
the predator/prey mass ratio around which the sigmoidal switch-on happens and
$u_l$ determines the steepness of that switch-on. Similarly $l_r$ is the log of
the predator/prey mass ratio around which the sigmoidal switch-off happens and
$u_r$ determines the steepness of that switch-off. Here is the plot of an 
example with switch-on around $l = 5$ and switch-off around $l = 25$.
```{r}
df <- tibble(
  l = seq(0, 30, length.out = 200),
  d = dtexp(l, 0.1, 5, 1, 25, 1)
)
ggplot(df) +
  geom_line(aes(l, d)) +
  geom_vline(xintercept = 5, colour = "grey") +
  geom_vline(xintercept = 25, colour = "grey")
```

The probability density $f_r(r)$ of the predator/prey mass ratio $r=e^l$ is
then a power-law distribution with exponent $\alpha - 1$ and sigmoidal cut-offs:
$$
f_r(r) \propto e^{-l}f_l(l)
= \frac{r^{\alpha - 1}}
{\left(1+\left(\frac{e^{l_l}}{r}\right)^{u_l}\right)
\left(1+\left(\frac{r}{e^{l_r}}\right)^{u_r}\right)}.
$$

#### Maximum likelihood fit
We estimate the 5 parameters by maximum likelihood estimation, which in this
case can only be done numerically.
```{r message=FALSE, warning=FALSE}
mle_texp <- function(df) {
  loglik <- function(alpha, ll, ul, lr, ur) {
    L <- dtexp(df$l, alpha, ll, ul, lr, ur)
    - sum(log(L) * df$weight_numbers)
  }
  mle2(loglik, start = list(
    alpha = 0.5,
    ll = min(df$l),
    lr = max(df$l),
    ul = 5,
    ur = 5))
}
est <- stomach %>% 
  group_modify(~ broom::tidy(mle_texp(.x))) %>% 
  select(Species, term, estimate) %>% 
  spread(term, estimate)
est
```
```{r}
grid = seq(0, 30, length.out = 200)
texpdens <- plyr::ddply(est, "Species", function(df) {
  data.frame(
    l = grid,
    Numbers = dtexp(grid, df$alpha, df$ll, df$ul, df$lr, df$ur),
    Biomass = dtexp(grid, df$alpha - 1, df$ll, df$ul, df$lr, df$ur)
  )}) %>% 
  gather(Type, Density, Numbers, Biomass)

ggplot(binned_stomach) +
  geom_col(aes(l, Density, fill = Type)) +
  facet_grid(Species ~ Type, scales = "free_y") +
    xlab("Log of predator/prey mass ratio") +
    geom_line(aes(l, Density, colour = Type), data = texpdens)
```


## Feeding kernel

Now we will discuss how the stomach content is related to the feeding kernel.

The stomach content is the result of a balance between the rate at which prey
items are ingested and the rate at which they are digested. So we start by
looking at ingestion and digestion rates.

### Ingestion
The rate of ingestion $I(w|w')$ is proportional to the product of the feeding
kernel $\phi(w'/w)$ and the prey number density $N_c(w)$:
$$
I(w|w') = i(w') \phi(w'/w)\ N_c(w)
$$
The proportionality constant $i(w')$ only depends on the predator size $w'$ and,
as we will see, we will not need it in our analysis. The prey number density
$N_c(w)$ is defined as usual such that $N_c(w)dw$ is the total number of
potential prey items with sizes between $w$ and $w+dw$.

In a multi-species model in which the predator has different interaction
strengths with different prey species, given by an interaction matrix
$\theta_{ij}$, the prey number density for a predator of species $i$ would be
obtained from the number densities $N_j(w)$ of individual prey species as
$$
N_c(w) = \sum_j \theta_{ij}N_j(w).
$$
Of course, we do not know the prey density that the predators experienced that
were sampled in the stomach observation. So the best we can do is to assume that
it was reasonably close to the Sheldon spectrum
$$
N_c(w) = n\ w^{-\lambda}
$$
with $\lambda \approx 2$.

### Digestion
Prey items will stay identifiable in the predator stomach only for a certain
period of time before they disintegrate. After that time they will not
contribute to the observation of prey items in the stomach. Of course we do not
know in detail the rate at which prey items are digested. However it is
reasonable to make the approximation that the rate at which a prey item has its
bodymass digested away scales with body size as $w^{2/3}$ because digestion acts
on the surface of the prey item and this surface scales approximately as
$w^{2/3}$. Let us assume that a prey item becomes unidentifiable when a fixed
percentage of its body mass is digested. The time until that has happened is
proportional to the body mass divided by the rate at which mass is digested:
$T\propto w/w^{2/3} = w^{1/3}$. The rate at which a prey item of size $w$
disintegrate is equal to the inverse of the disintegration time. Denoting that
rate by $D(w)$ we have
$$
D(w) = d\ w^{-1/3}.
$$
The proportionality constant $d$ will not be important in what follows.

### Balance equation
We describe the size distribution of prey in stomachs of predators of size $w'$
by the density function $N_w(w|w')$. 

The observed stomach content is such that the rate at which prey items of a
particular size are ingested is equal to the rate at which such items
disintegrates due to digestion, $I(w|w') = D(w)N_w(w|w')$. Using our expressions
for these rates we obtain
$$
i(w') \phi(w'/w)\ n\ w^{-\lambda} = d\ w^{-1/3} N_w(w|w').
$$
This we can solve for the feeding kernel:
$$
\phi(w'/w) = \frac{d}{i(w')\,n}\ w^{\lambda - 1/3}N_w(w|w').
$$
Using further that
$$
N_w(w|w') \propto e^lf_l(l)
$$
where $l=\log(w'/w)$ we find
$$
\phi(w'/w) \propto e^{(4/3 - \lambda)l}f_l(l)
$$
This tells us that if the stomach distribution $f_l(l)$ is described by the
truncated exponential distribution with rate $\alpha$, the feeding kernel as a
function of $l$ is described by the truncated exponential distribution with rate
$\alpha + 4/3 - \lambda$.
```{r}
est %>% 
  mutate(exp = alpha + 4/3 - 2.05)
```


If the stomach distribution $f_l(l)$ is the normal distribution with mean $\mu$
and variance $\sigma^2$ then the feeding kernel as a function of $l$ is also a
Gaussian with the same variance and with mean 
$\mu + (4/3 - \lambda)\sigma^2$.


## Barnes dataset

Reading in the data
```{r message=FALSE}
stomach_full <-
    read_tsv("Predator_and_prey_body_sizes_in_marine_food.csv",
             na = c("", "n/a"),
             guess_max = 10000)
stomach_full
```
We select only the relevant columns and remove predator species with fewer than 1000 observations.
```{r}
stomach <- stomach_full %>% 
    select(Species = `Predator common name`,
           wpredator = `SI predator mass`,
           wprey = `SI prey mass`) %>% 
    group_by(Species) %>% 
    filter(n() > 1000) %>% 
    mutate(Nprey = 1,
           l = log(wpredator / wprey),
           weight_numbers = Nprey / sum(Nprey),
           weight_biomass = Nprey * wprey / sum(Nprey * wprey))
stomach
```
This leaves
```{r}
unique(stomach$Species)
```
For later purposes we also bin the observations.
```{r}
no_bins <- 30  # Number of bins
binsize <- (max(stomach$l) - min(stomach$l)) / (no_bins - 1)
breaks <- seq(min(stomach$l) - binsize/2,
              by = binsize, length.out = no_bins + 1)
binned_stomach <- stomach %>% 
    # bin data
    mutate(cut = cut(l, breaks = breaks, right = FALSE,
                     labels = FALSE)) %>% 
    group_by(Species, cut) %>% 
    summarise(Numbers = sum(Nprey), 
              Biomass = sum(Nprey * wprey)) %>% 
    # normalise
    mutate(Numbers = Numbers / sum(Numbers) / binsize,
           Biomass = Biomass / sum(Biomass) / binsize)  %>%
    # column for predator/prey size ratio
    mutate(l = map_dbl(cut, function(idx) breaks[idx] + binsize/2)) %>% 
    gather(key = "Type", value = "Density", Numbers, Biomass)
```

We take a look at the hexplots to see that the assumption that the log predator/prey
mass ratio is independent of the predator size is not unreasonable.
```{r}
stomach %>% ggplot(aes(log(wpredator), l)) +
    geom_hex(bins = 15) +
    scale_fill_viridis_c(trans = "log", breaks = c(2, 10, 50, 250, 1250)) +
    stat_smooth(method = 'loess') +
    facet_wrap(~Species, scales = "free_x")
```

I think it is fair to say that these plots do not exhibit any systematic trend.

### Gaussian sometimes fits
Next we fit normal densities:
```{r}
grid <- seq(0, max(stomach$l), length = 100)
normaldens <- plyr::ddply(stomach, "Species", function(df) {
  data.frame( 
    l = grid,
    density = dnorm(grid, mean(df$l), sd(df$l))
  )
})

ggplot(stomach) +
    geom_density(aes(l, weight = weight_numbers), fill = "#00BFC4") +
    facet_wrap(~Species, scales = "free_y", ncol = 4) +
    xlab("Log of predator/prey mass ratio")  +
    geom_line(aes(l, density), data = normaldens,
                  colour = "blue")
```

This does not look too bad at first sight. However we now know to also look
at the biomass distribution. We plot the normal density arising from the
above fits on top of the observed biomass distribution.
```{r}
grid <- seq(0, max(stomach$l), length = 100)
shifted_normaldens <- plyr::ddply(stomach, "Species", function(df) {
  data.frame( 
    l = grid,
    density = dnorm(grid, mean(df$l) - sd(df$l)^2, sd(df$l))
  )
})

ggplot(stomach) +
    geom_density(aes(l, weight = weight_biomass), fill = "#F8766D") +
    facet_wrap(~Species, scales = "free_y", ncol = 4) +
    xlab("Log of predator/prey mass ratio") +
    ylab("Biomass density") +
    geom_line(aes(l, density), data = shifted_normaldens,
                  colour = "red")
```

This looks o.k. for some species, but is totally wrong for others, in particular
the atlantic cod and the silver hake, where the normal distribution incorrectly
predicts most of the biomass to be at sizes larger than the predator size
(negative log of predator/prey mass ratio).

### Truncated exponential
```{r}
mle_texp <- function(df) {
  loglik <- function(alpha, ll, ul, lr, ur) {
    L <- dtexp(df$l, alpha, ll, ul, lr, ur)
    - sum(log(L) * df$weight_numbers)
  }
  mle2(loglik, start = list(
    alpha = 0.5,
    ll = min(df$l),
    lr = max(df$l),
    ul = 5,
    ur = 5))
}
est <- stomach %>% 
  group_modify(~ broom::tidy(mle_texp(.x))) %>% 
  select(Species, term, estimate) %>% 
  spread(term, estimate)
est
```
```{r fig.height=15}
grid = seq(0, 18, length.out = 200)
texpdens <- plyr::ddply(est, "Species", function(df) {
  data.frame(
    l = grid,
    Numbers = dtexp(grid, df$alpha, df$ll, df$ul, df$lr, df$ur),
    Biomass = dtexp(grid, df$alpha - 1, df$ll, df$ul, df$lr, df$ur)
  )}) %>% 
  gather(Type, Density, Numbers, Biomass)

ggplot(binned_stomach) +
  geom_col(aes(l, Density, fill = Type)) +
  facet_grid(Species ~ Type, scales = "free_y") +
    xlab("Log of predator/prey mass ratio") +
    geom_line(aes(l, Density, colour = Type), data = texpdens)
```


## Summary

## Appendix

### More complicated digestion
This still needs work but is probably irrelevant anyway.

As prey items are digested, they loose biomass. Thus the observed size of a prey
item will not be the original size at which it was ingested. Let us denote by
$S(w)$ the rate at which a prey item of size $w$ looses weight due to digestion.
We have already discussed that due to the fact that digestion acts upon the
surface of the prey item, we can make the approximation that
$S(w)= s w^{2/3}$, with the proportionality constant $S$ depending only on
the predator. 

The density of observed prey items at a particular size will increase when such
a prey item is ingested or if a larger prey item is digested down to this size,
and the density will decrease when a prey item is digested down to a smaller
size or becomes unidentifiable. This leads to the partial differential equation
$$
\frac{N_w(w|w')}{dt} = I(w|w') +\partial_w\left(D(w|w')N_w(w|w')\right)
- U(w|w')N_w(w|w').
$$
We assume that on average the stomach observations reflect the steady state
with $dN/dt=0$. We now substitute our expressions for the various rates into
the resulting balance equation:
$$
0 = i(w') \phi(w'/w)\ w^{-\lambda} 
+ \frac{\partial}{\partial_w}\left(d(w')w^{2/3}N_w(w|w')\right)
- u(w')w^{-1/3}N_w(w|w')
$$
